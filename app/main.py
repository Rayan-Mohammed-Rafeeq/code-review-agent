from __future__ import annotations

import logging
import os
import pathlib
import re
import urllib.parse

import requests
from fastapi import APIRouter, Body, Depends, FastAPI, File, Header, HTTPException, UploadFile
from fastapi.responses import FileResponse, JSONResponse
from fastapi.staticfiles import StaticFiles
from starlette.middleware.cors import CORSMiddleware

from app import deps, firebase_auth
from app.ai_agent import CodeReviewAgent
from app.deps import get_agent
from app.firebase_debug import get_token_hints
from app.logging_config import configure_logging
from app.models import ReviewRequest, ReviewResponse
from app.routers.review_v2 import router as review_v2_router
from app.settings import Settings
from app.strict_format import format_strict_findings

configure_logging()

logger = logging.getLogger("code_review_agent")

APP_VERSION = "1.0.0"

app = FastAPI(title="Code Review Agent", version=APP_VERSION)

# --- CORS ---
# Configure allowed origins via env vars.
#
# Render/Vercel notes:
# - If your frontend is on Vercel, set CODE_REVIEW_CORS_ORIGINS to the exact deployed URL(s),
#   e.g. https://coderagent.vercel.app
# - If you also want to allow Vercel preview deployments, set CODE_REVIEW_CORS_ORIGIN_REGEX to
#   something like: ^https://.*\\.vercel\\.app$
#
# Examples:
#   CODE_REVIEW_CORS_ORIGINS=https://my-ui.vercel.app,https://my-ui.custom-domain.com
#   CODE_REVIEW_CORS_ORIGIN_REGEX=^https://.*\\.vercel\\.app$
_cors_origins_raw = os.getenv("CODE_REVIEW_CORS_ORIGINS", "").strip()
_cors_origin_regex = os.getenv("CODE_REVIEW_CORS_ORIGIN_REGEX", "").strip()

if _cors_origins_raw or _cors_origin_regex:
    _cors_origins = [o.strip() for o in _cors_origins_raw.split(",") if o.strip()]
    app.add_middleware(
        CORSMiddleware,
        allow_origins=_cors_origins,
        allow_origin_regex=_cors_origin_regex or None,
        allow_credentials=True,
        allow_methods=["*"],
        allow_headers=["*"],
    )
else:
    logger.warning(
        "CORS is not configured. Set CODE_REVIEW_CORS_ORIGINS (and optionally CODE_REVIEW_CORS_ORIGIN_REGEX) "
        "to allow browser clients.",
        extra={"env": "CODE_REVIEW_CORS_ORIGINS"},
    )

# v2 routes
app.include_router(review_v2_router)

# Duplicate v2 under /api/v2/... for frontend compatibility

_api = APIRouter(prefix="/api")
_api.include_router(review_v2_router)
app.include_router(_api)


# --- Preflight (OPTIONS) helpers ---
# Some hosting/proxy setups can surface 405s for OPTIONS even when CORSMiddleware is present.
# Explicitly answering OPTIONS keeps browser clients working reliably.
@app.options("/v2/review/file")
def options_v2_review_file():
    return JSONResponse(content=None, status_code=200)


@app.options("/api/v2/review/file")
def options_api_v2_review_file():
    return JSONResponse(content=None, status_code=200)


# --- Frontend (React) static serving ---
# In production, we serve the built SPA from frontend/dist/spa.
# IMPORTANT: mounting StaticFiles at '/' can shadow/alter routing in tests.
# Gate it behind an env var so API routes remain deterministic.
_FRONTEND_DIST_DIR = pathlib.Path(__file__).resolve().parents[1] / "frontend" / "dist" / "spa"
if _FRONTEND_DIST_DIR.exists() and (os.getenv("SERVE_SPA", "").strip().lower() in {"1", "true", "yes"}):
    # Mount hashed assets (e.g. /assets/*) and other static files generated by Vite.
    app.mount("/", StaticFiles(directory=str(_FRONTEND_DIST_DIR), html=True), name="spa")


def _normalize_github_repo_url(url: str) -> str:
    u = (url or "").strip()
    if not u:
        raise ValueError("GitHub repo URL is required")

    parsed = urllib.parse.urlparse(u)
    if parsed.scheme not in ("http", "https"):
        raise ValueError("GitHub repo URL must start with http(s)")

    host = (parsed.netloc or "").lower()
    if host not in ("github.com", "www.github.com"):
        raise ValueError("Only github.com URLs are supported")

    path = (parsed.path or "").strip("/")
    parts = [p for p in path.split("/") if p]
    if len(parts) < 2:
        raise ValueError("GitHub repo URL must look like https://github.com/<owner>/<repo>")

    owner, repo = parts[0], parts[1]
    repo = re.sub(r"\.git$", "", repo, flags=re.IGNORECASE)
    return f"https://github.com/{owner}/{repo}"


@app.post("/review", response_model=ReviewResponse)
async def review_json_endpoint(
    payload: ReviewRequest = Body(...),
    agent: CodeReviewAgent = Depends(get_agent),
    settings: Settings = Depends(deps.get_settings_dep),
):
    """JSON-only review endpoint.

    Accepts:
      {"code": "...", "language": "python", "filename": "optional.py"}

    If `strict=true`, the response also includes `strict_findings`, a human-readable
    findings string in a fixed format.
    """
    lang = (payload.language or "python").strip().lower()

    # IMPORTANT: only enforce LLM configuration when a real LLM provider is enabled.
    # In offline mode (LLM_PROVIDER=none) the LLM client returns an empty issues list.
    if (settings.llm_provider or "openai").lower().strip() != "none":
        _ensure_llm_configured(settings)

    try:
        compressed, static_dict, issues = await agent.review(
            code=payload.code,
            filename=payload.filename or "input.py",
            language=lang,
            strict=bool(payload.strict),
        )
    except RuntimeError as e:
        logger.warning("Review failed due to runtime error", extra={"source_filename": payload.filename})
        raise HTTPException(status_code=502, detail=str(e))
    except HTTPException:
        raise
    except Exception as e:
        logger.exception("Unexpected error during review", extra={"source_filename": payload.filename})
        # Surface a small, safe error message for debuggability.
        raise HTTPException(status_code=502, detail=f"Review failed: {type(e).__name__}: {e}")

    strict_findings = format_strict_findings(issues) if payload.strict else None
    resp_model = ReviewResponse(
        compressed_context=compressed,
        static_analysis=static_dict,
        issues=issues,
        strict_findings=strict_findings,
    )

    out = resp_model.model_dump()
    return JSONResponse(out)


@app.post("/api/review", response_model=ReviewResponse)
async def review_json_endpoint_api(
    payload: ReviewRequest = Body(...),
    agent: CodeReviewAgent = Depends(get_agent),
    settings: Settings = Depends(deps.get_settings_dep),
):
    return await review_json_endpoint(payload=payload, agent=agent, settings=settings)


@app.post("/review/file", response_model=ReviewResponse)
async def review_file_endpoint(
    file: UploadFile = File(...),
    agent: CodeReviewAgent = Depends(get_agent),
    settings: Settings = Depends(deps.get_settings_dep),
):
    """Multipart file-upload review endpoint."""
    try:
        code, filename = await _read_code_from_file(file=file)
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))

    # Pass settings explicitly so tests can monkeypatch app.main.get_settings
    # and have it deterministically reflected here.
    if (settings.llm_provider or "openai").lower().strip() != "none":
        _ensure_llm_configured(settings)

    try:
        compressed, static_dict, issues = await agent.review(code=code, filename=filename, language="python")
    except RuntimeError as e:
        logger.warning("Review failed due to runtime error", extra={"source_filename": filename})
        raise HTTPException(status_code=502, detail=str(e))
    except HTTPException:
        raise
    except Exception as e:
        logger.exception("Unexpected error during review", extra={"source_filename": filename})
        raise HTTPException(status_code=502, detail=f"Review failed: {type(e).__name__}: {e}")

    resp_model = ReviewResponse(compressed_context=compressed, static_analysis=static_dict, issues=issues)

    out = resp_model.model_dump()
    return JSONResponse(out)


@app.post("/review/github", response_model=ReviewResponse)
async def review_github_endpoint(
    payload: dict = Body(...),
    agent: CodeReviewAgent = Depends(get_agent),
    settings: Settings = Depends(deps.get_settings_dep),
):
    """Review a GitHub repo by fetching a single file from it.

    Accepts:
      {"repo_url": "https://github.com/owner/repo", "path": "path/in/repo.py", "ref": "main", "strict": false}

    Note: This uses the unauthenticated raw GitHub endpoint. Large/complex repos
    are intentionally out-of-scope for now.
    """
    repo_url = _normalize_github_repo_url(payload.get("repo_url") or "")
    path = (payload.get("path") or "").strip() or "README.md"
    ref = (payload.get("ref") or "").strip() or "main"
    strict = bool(payload.get("strict") or False)

    if not path.endswith(".py"):
        raise HTTPException(status_code=400, detail="Only .py files are supported")

    parsed = urllib.parse.urlparse(repo_url)
    owner, repo = [p for p in parsed.path.strip("/").split("/") if p][:2]
    raw_url = f"https://raw.githubusercontent.com/{owner}/{repo}/{ref}/{path}"

    # This endpoint requires network access. Keep it deterministic for judges:
    # if offline mode is enabled, fail fast with a clear message.
    if (settings.llm_provider or "openai").lower().strip() == "none":
        raise HTTPException(status_code=400, detail="GitHub review is disabled in offline mode (LLM_PROVIDER=none)")

    _ensure_llm_configured(settings)

    try:
        r = requests.get(raw_url, timeout=15)
    except requests.exceptions.RequestException as e:
        raise HTTPException(status_code=502, detail=f"Failed to fetch GitHub raw file: {e.__class__.__name__}")

    if r.status_code != 200:
        raise HTTPException(status_code=400, detail=f"Failed to fetch file from GitHub (HTTP {r.status_code})")

    code = r.text
    filename = os.path.basename(path) or "input.py"

    try:
        compressed, static_dict, issues = await agent.review(
            code=code, filename=filename, language="python", strict=strict
        )
    except RuntimeError as e:
        raise HTTPException(status_code=502, detail=str(e))

    resp_model = ReviewResponse(
        compressed_context=compressed,
        static_analysis=static_dict,
        issues=issues,
        strict_findings=(format_strict_findings(issues) if strict else None),
    )

    out = resp_model.model_dump()
    return JSONResponse(out)


async def _read_code_from_file(*, file: UploadFile) -> tuple[str, str]:
    raw = await file.read()
    try:
        code = raw.decode("utf-8")
    except UnicodeDecodeError as e:
        raise ValueError("Uploaded file must be UTF-8 encoded") from e
    filename = file.filename or "input.py"
    if not filename.endswith(".py"):
        filename = filename + ".py"
    return code, filename


# Backwards-compatible helper kept for internal use only.
async def _read_code(*, payload: ReviewRequest | None, file: UploadFile | None) -> tuple[str, str]:
    if file is not None:
        return await _read_code_from_file(file=file)

    if payload is None:
        raise ValueError("Provide either JSON body with 'code' or upload a file")

    return payload.code, payload.filename or "input.py"


@app.get("/healthz")
def healthz():
    # Avoid secrets: only report whether Firebase verification is configured and initialized.
    fb_configured = bool(
        os.getenv("FIREBASE_SERVICE_ACCOUNT_JSON")
        or os.getenv("FIREBASE_SERVICE_ACCOUNT_FILE")
        or os.path.exists(os.path.join(os.getcwd(), "firebase-service-account.json"))
    )
    fb_initialized = bool(firebase_auth._init_admin())  # type: ignore[attr-defined]
    return JSONResponse(
        {
            "ok": True,
            "service": "code-review-agent",
            "version": APP_VERSION,
            "firebase_configured": fb_configured,
            "firebase_initialized": fb_initialized,
        }
    )


@app.get("/configz")
def configz():
    # Never return the key itself.
    llm_key_set = bool(os.getenv("LLM_API_KEY"))
    scaledown_key_set = bool(os.getenv("SCALEDOWN_API_KEY"))
    scaledown_enabled_raw = os.getenv("SCALEDOWN_ENABLED")
    scaledown_enabled = None if scaledown_enabled_raw is None else (scaledown_enabled_raw.strip().lower() in {"1", "true", "yes", "y", "on"})

    fb_source = firebase_auth._cred_source()  # type: ignore[attr-defined]
    fb_initialized = bool(firebase_auth._init_admin())  # type: ignore[attr-defined]
    return JSONResponse(
        {
            "llm_api_key_set": llm_key_set,
            "llm_base_url": os.getenv("LLM_BASE_URL"),
            "llm_model": os.getenv("LLM_MODEL"),
            "scaledown": {
                "api_key_set": scaledown_key_set,
                "enabled": scaledown_enabled,
            },
            "firebase": {
                "credential_source": fb_source,
                "initialized": fb_initialized,
            },
        }
    )


@app.get("/auth/firebase_debug")
def firebase_debug(authorization: str | None = Header(default=None)):
    """Debug endpoint to diagnose Firebase token/project mismatches.

    Returns *unverified* JWT payload hints (aud/iss/project) and backend Firebase init status.
    This is intended for local debugging. Do not rely on it for security decisions.

    Note: this *does not* authenticate anyone. It's a diagnostics endpoint only.
    """
    token = None
    if authorization and str(authorization).lower().startswith("bearer "):
        token = str(authorization).split(" ", 1)[1].strip() or None

    hints = get_token_hints(token or "") if token else None

    return JSONResponse(
        {
            "firebase": {
                "credential_source": firebase_auth._cred_source(),  # type: ignore[attr-defined]
                "initialized": bool(firebase_auth._init_admin()),  # type: ignore[attr-defined]
            },
            "token_hints": (hints.__dict__ if hints else None),
        }
    )


def _ensure_llm_configured(s=None) -> None:
    """Raise a 400 with guidance when the app isn't configured to call an LLM.

    The backend supports LLM_PROVIDER=none for offline mode (it will return an
    empty issues list). For any other provider, we require at least an API key.

    Base URL and model have sensible defaults (see Settings) but can still be
    overridden via environment variables.
    """
    if s is None:
        s = deps.get_settings_dep()
    provider = (s.llm_provider or "openai").lower().strip()
    if provider == "none":
        return

    missing: list[str] = []
    if not (s.llm_api_key or "").strip():
        missing.append("LLM_API_KEY")

    if missing:
        raise HTTPException(
            status_code=400,
            detail=(
                "LLM is not configured (missing: "
                + ", ".join(missing)
                + "). Set these environment variables and restart the API, or set LLM_PROVIDER=none to run without LLM calls."
            ),
        )


# If we are serving the SPA, make sure client-side routes resolve to index.html.
# NOTE: This must be declared after API routes so it doesn't shadow them.
if _FRONTEND_DIST_DIR.exists() and (os.getenv("SERVE_SPA", "").strip().lower() in {"1", "true", "yes"}):

    @app.get("/{full_path:path}")
    def spa_fallback(full_path: str):
        # Don't intercept API routes.
        if full_path.startswith("api/") or full_path.startswith("docs") or full_path.startswith("openapi"):
            raise HTTPException(status_code=404)

        index_path = _FRONTEND_DIST_DIR / "index.html"
        if not index_path.exists():
            raise HTTPException(status_code=404, detail="Frontend not built")
        return FileResponse(str(index_path))

# Log basic LLM configuration at startup (never log the key value)
try:
    logger.info(
        "LLM config",
        extra={
            "provider": os.getenv("LLM_PROVIDER"),
            "base_url": os.getenv("LLM_BASE_URL"),
            "model": os.getenv("LLM_MODEL"),
            "llm_api_key_set": bool(os.getenv("LLM_API_KEY")),
        },
    )
except Exception:
    pass
