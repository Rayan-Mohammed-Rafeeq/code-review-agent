# Copy to .env and fill in
LLM_API_KEY=o2rP60dvG186QmyYQlOaC4BB6wyaYpc72h7uItVU
LLM_BASE_URL=https://api.openai.com/v1
LLM_MODEL=gpt-4o-mini
from __future__ import annotations

from enum import Enum
from typing import Any, Literal, Optional

from pydantic import BaseModel, Field


class Severity(str, Enum):
    high = "high"
    medium = "medium"
    low = "low"


class Category(str, Enum):
    security = "security"
    bug = "bug"
    performance = "performance"
    style = "style"


class Issue(BaseModel):
    severity: Severity
    category: Category
    description: str = Field(min_length=1)
    suggestion: str = Field(min_length=1)
    location: Optional[str] = None
    metadata: dict[str, Any] = Field(default_factory=dict)


class ReviewRequest(BaseModel):
    code: Optional[str] = Field(default=None, description="Raw source code")
    filename: Optional[str] = Field(default="input.py")


class ReviewResponse(BaseModel):
    compressed_context: str
    static_analysis: dict[str, Any]
    issues: list[Issue]


# OpenAI-compatible chat response schema we want from the LLM.
LLM_JSON_SCHEMA: dict[str, Any] = {
    "type": "object",
    "properties": {
        "issues": {
            "type": "array",
            "items": {
                "type": "object",
                "properties": {
                    "severity": {"type": "string", "enum": ["high", "medium", "low"]},
                    "category": {"type": "string", "enum": ["security", "bug", "performance", "style"]},
                    "description": {"type": "string"},
                    "suggestion": {"type": "string"},
                    "location": {"type": ["string", "null"]},
                },
                "required": ["severity", "category", "description", "suggestion"],
                "additionalProperties": False,
            },
        }
    },
    "required": ["issues"],
    "additionalProperties": False,
}

